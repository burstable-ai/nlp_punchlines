{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "253bbd98-f24f-4957-9d40-ecb984029f77",
   "metadata": {},
   "source": [
    "<div style=background-color:#EEEEFF>\n",
    "\n",
    "## 5. How well does our fine-tuned Joke Generator perform?\n",
    "\n",
    "Finally!  We have an NLP generator model that has been specifically trained to supply punchlines for jokes.  Let's see how well it does compared to the pre-trained GPT-2 model we started with."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a583a74-5680-490d-9321-a6624c2272fa",
   "metadata": {},
   "source": [
    "<div style=background-color:#EEEEFF>\n",
    "\n",
    "We'll start by loading our test data.  \n",
    "    \n",
    "Recall: these are jokes that were *not* used in the fine-tune training of our generator.  We also did not use them when training our Punchline Classifier (although we did use them to *test* our classifier), so neither the generator models nor the classifier have been trained on these jokes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c00a73-3975-4bf7-9a68-6de154146375",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import data_tools as dtools\n",
    "\n",
    "test = pd.read_csv('data/short_jokes_test.csv')\n",
    "\n",
    "print(test.shape)\n",
    "print(test.columns)\n",
    "test.iloc[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10227eda-9838-424a-a4bc-e7afee924ed3",
   "metadata": {},
   "source": [
    "<div style=background-color:#EEEEFF>\n",
    "\n",
    "As in [2.FakePunchlines](2.FakePunchlines.ipynb), we'll reformat the jokes into the Q/A format:\n",
    "> \"Question: [setup text, ends with '?'] Answer: [punchline text]\"    \n",
    "    \n",
    "We'll then strip everything after \"Answer:\" to generate a prompt that we will pass to the generator for it to supply a punchline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdac8d81-c095-4e82-8a1f-9bc012a4c134",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.DataFrame(test[(test['setup'].apply(lambda x: type(x)==str)) & (test['punchline'].apply(lambda x: type(x)==str))])\n",
    "test['full_qa'] = [dtools.joke_as_qa(row['setup'],row['punchline']) for (i,row) in test.iterrows()]\n",
    "test['prompt'] = test['full_qa'].apply(lambda x: x[:x.find('Answer:')+len('Answer:')].strip())\n",
    "\n",
    "pd.options.display.max_colwidth = None   # don't truncate the column text\n",
    "test.iloc[:3]['prompt']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324782ec-2717-4826-9438-e0de33fa1f5a",
   "metadata": {},
   "source": [
    "<div style=background-color:#EEEEFF>\n",
    "\n",
    "Now let's load our generator models.  We'll load both the original, pre-trained GPT-2 model and one that we have fine-tuned through 10 epochs of training on our full jokes training dataset. \n",
    "    \n",
    "We can use the same GPT-2 tokenizer for both models, since it's the one that was used to encode the data for both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c75f6a-e059-4371-83ac-2759142b4c48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import model_tools as mtools\n",
    "from torch import load\n",
    "\n",
    "# Load the original GPT-2 model and its tokenizer, and put it on the GPU\n",
    "checkpoint = mtools.load_checkpoint('gpt2')\n",
    "tokenizer = mtools.load_tokenizer(checkpoint)\n",
    "model_gpt2 = mtools.load_model(checkpoint)    \n",
    "model_gpt2.to(mtools.get_device(use_gpu=True))\n",
    "print('Original model is on {}'.format(model_gpt2.device))\n",
    "\n",
    "# Load our fine-tuned 10-epoch model and put it on the GPU\n",
    "model_10e = load('models/JokeGen_gpt2.pt')\n",
    "print('Fine-tuned model10 is on {}'.format(model_10e.device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7163b8e5-b5cd-40af-996b-aa6a7dbac6e1",
   "metadata": {},
   "source": [
    "<div style=background-color:#EEEEFF>\n",
    "\n",
    "Then we pass the test prompts to the original GPT-2 model, and to our fine-tuned 10-epoch model, for them to generate punchlines.  \n",
    "    \n",
    "Text generation is somewhat slow, so we'll just generate punchlines for the first 1,000 test jokes.  This will take a few minutes (you can watch the progress bar, or go make a cup of tea while you wait)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5023fdb4-2bc6-4eb6-8bda-30c8a00b24f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_gpt2 = mtools.generate(model_gpt2, tokenizer, list(test.iloc[:1000]['prompt']))\n",
    "generated_10e = mtools.generate(model_10e, tokenizer, list(test.iloc[:1000]['prompt']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3684c98-b3f6-4b70-b262-fc44d6094993",
   "metadata": {},
   "source": [
    "<div style=background-color:#EEEEFF>\n",
    "\n",
    "Let's look at the first punchline from each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c178f13-4a75-447b-b6c6-5eecf6446d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generated_gpt2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c55cd47-ec88-47a5-a9cb-4f4f43e77e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generated_10e[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62067f2-9e40-44cc-b227-bfcb160349ca",
   "metadata": {},
   "source": [
    "<div style=background-color:#EEEEFF>\n",
    "\n",
    "If you're running these notebooks yourself, you'll have different output from what I see.  Just so that we're on the same page, here's the copy-and-paste of the output I got for pre-trained GPT-2:\n",
    "\n",
    "> <|endoftext|>Question: Did you know Google now has a platform for recording your bowel movements? Answer: No, you don't. Just like Google Now only works on those kinds of things.\n",
    "> \n",
    "> Answer: Sure. We use this company to track\n",
    "\n",
    "And for the fine-tuned model:\n",
    "    \n",
    "> <|endoftext|>Question: Did you know Google now has a platform for recording your bowel movements? Answer: \"Sphinx\"<|endoftext|>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82bf1999-c900-4d6d-a34f-89b9baa8e72a",
   "metadata": {},
   "source": [
    "<div style=background-color:#EEEEFF>\n",
    "\n",
    "There are a number of things to note here.\n",
    "\n",
    "1. Pre-trained GPT-2 almost never generates an \"<|endoftext|>\" token; it just keeps blathering on until it hits the 60-token limit we imposed.  In contrast, the fine-tuned model often (but not always) *does* generate an \"<|endoftext|>\" token.\n",
    "    \n",
    "2. Pre-trained GPT-2 has a tendency to repeat itself; it often has generated an second \"Answer:...\" string.  Sometimes it will also generate additional \"Question:...\"  strings.  \n",
    "    \n",
    "3. The fine-tuned model has a short, pithy punchline, much like a real joke.  (Whether it makes sense in context, or whether it's funny, are separate questions.)\n",
    "    \n",
    "4. The fine-tuned model punchline almost works---could be sort of a pun on \"Google Sheets\" and \"Sphincter\"?  I think this is a coincidence.  (From all the other examples I've looked at, I don't think GPT-2 is capable of doing puns.  We'll discuss this more in a future notebook.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5564e75-3699-48bb-a6ba-3d8fbe6081fa",
   "metadata": {},
   "source": [
    "<div style=background-color:#EEEEFF>\n",
    "\n",
    "Let's see if this pattern persists across a larger number of punchlines, again comparing out-of-the-box GPT-2 with the fine-tuned 10-epoch model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb9253c-772d-4996-8792-23148c0bd308",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "def clean_qa(text,eot=None):\n",
    "    if eot is not None:\n",
    "        text = text.replace(eot,'').strip()\n",
    "    text = text.replace('\\n',' ')\n",
    "    q = text[:text.find('Answer:')]\n",
    "    q = q[len('Question: '):].strip()\n",
    "    a = text[text.find('Answer:'):]\n",
    "    a = a[len('Answer: '):].strip()\n",
    "    return q, a\n",
    "\n",
    "eot = '<|endoftext|>'\n",
    "for i in range(5,10,1):\n",
    "    print('{:>13}: {}'.format('Joke Setup',clean_qa(test.iloc[i]['full_qa'],eot)[0]))\n",
    "    print('{:>10}'.format('--Punchlines--'))\n",
    "    punch_dict = {'      Real':clean_qa(test.iloc[i]['full_qa'],eot)[1],\n",
    "                  '     GPT-2':clean_qa(generated_gpt2[i],eot)[1],\n",
    "                  'Fine-tuned':clean_qa(generated_10e[i],eot)[1]}\n",
    "    pprint(punch_dict)\n",
    "    print('===========================')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f0b6dc-92c4-4d00-ad95-08c811de184b",
   "metadata": {},
   "source": [
    "<div style=background-color:#EEEEFF>\n",
    "\n",
    "In general, the GPT-2 answers are long and rambling.  They are often on-topic, but they completely miss the Q/A style of the joke format.  This is not surprising---GPT-2 is trained on a wide variety of text.\n",
    "    \n",
    "The fine-tuned model does better.  Its first sentence of answer is usually short, pithy, and has the right cadence for a joke.  However, it too often continues rambling on, often inserting additional \"Answer:\" text, or even adding an additional \"Question:\".  \n",
    "    \n",
    "Let's quantify the different between vanilla GPT-2 and our fine-tuned model by exploring several different metrics, informed by this quick look at the data:\n",
    "    \n",
    "1. How often does the generator produce an \"<|endoftext|>\" token, versus rambling on until stopped by our generator token limit?\n",
    "    \n",
    "2. How long are the generated answers?\n",
    "    \n",
    "3. How often are the punchline \"answers\" just more questions?\n",
    "    \n",
    "4. How often does it insert an additional \"Answer:\" or \"Question:\" into the punchline?\n",
    "    \n",
    "5. How often can the generator fool our BERT-based joke classifier into thinking it's a human-generated joke?\n",
    "\n",
    "Let's go through each of these."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435a0c4c-c7f3-4029-91e9-9b8223b777a6",
   "metadata": {},
   "source": [
    "<div style=background-color:#EEEEFF>\n",
    "\n",
    "### 1. How often does the generator produce an \"<|endoftext|>\" token?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4217974c-5f78-4edb-818d-03ae1f9b804f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count number of entries that generated a final EOT token (all of them begin with an EOT token by construction)\n",
    "eot = '<|endoftext|>'\n",
    "n_eot_gpt2 = len([x for x in generated_gpt2 if x.count(eot)==2])\n",
    "n_eot_10e = len([x for x in generated_10e if x.count(eot)==2])\n",
    "\n",
    "print('Of the 1,000 jokes for which we generated punchlines:')\n",
    "print('  {:>4} had EOT tokens generated by pre-trained GPT-2'.format(n_eot_gpt2))\n",
    "print('  {:>4} had EOT tokens generated by fine-tuned GPT-2 (10 epochs)'.format(n_eot_10e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6126ac99-e834-4941-b5b0-6f7e52dd6cbd",
   "metadata": {},
   "source": [
    "<div style=background-color:#EEEEFF>\n",
    "\n",
    "A human-generated punchline has a stopping point.  \n",
    "    \n",
    "Pre-trained GPT-2 basically doesn't know when to stop: it generated an \"<|endoftext|>\" token only ~10 out of 1,000 times in our dataset (your numbers may vary slightly from this).  \n",
    "    \n",
    "Our fine-tuned model does better: it generates an \"<|endoftext|>\" token about half the time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4486c063-b1b1-477c-849b-2f213edcf8a8",
   "metadata": {},
   "source": [
    "<div style=background-color:#EEEEFF>\n",
    "\n",
    "### 2. How long are the generated answers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04beb23-5c87-4f28-a9fe-feb63d5e916d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tally length (in words) of each output text\n",
    "len_gpt2 = [len(clean_qa(x,eot)[1].split()) for x in generated_gpt2]\n",
    "len_10e = [len(clean_qa(x,eot)[1].split()) for x in generated_10e]\n",
    "len_true = [len(clean_qa(x,eot)[1].split()) for x in test['full_qa'].iloc[:1000]]\n",
    "\n",
    "# Make a histogram of the punchline lengths for Human, GPT-2, and Fine-tuned punchlines\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "bins = np.arange(35)\n",
    "plt.hist(len_true[:1000],bins=bins,label='Human',\n",
    "                      linewidth=2, edgecolor='blue', hatch='\\\\\\\\\\\\\\\\',fill=False)\n",
    "plt.hist(len_gpt2,bins=bins,label='GPT2',color='r',\n",
    "         linewidth=2, edgecolor='r', hatch='//',fill=False)\n",
    "plt.hist(len_10e,bins=bins,alpha=0.5,label='FT: 10 epochs',color='g')\n",
    "plt.xlabel('# of words in punchline')\n",
    "plt.ylabel('# of occurences')\n",
    "l = plt.legend(bbox_to_anchor=(1, 1), loc='upper left', ncol=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d587cea1-f109-41e8-8fe3-583189a78d9d",
   "metadata": {},
   "source": [
    "<div style=background-color:#EEEEFF>\n",
    "\n",
    "Human punchlines (blue histogram above) tend to be succint.\n",
    "    \n",
    "We specifically chose jokes that had a Q/A format and had punchlines that were no more than 20 words long, so it's no surprise that our human-generated punchlines are short and max out at 20 words in length.  Notice, though, that fewer jokes have long punchlines--the histogram is already trailing off before we truncate at 20, so this is a true representation of most human joke punchlines for Q/A-style jokes, not just a result of our artificial cut.  \n",
    "    \n",
    "In contrast, GPT-2 generates punchlines (red histogram above) that ramble on too long.  Since pre-trained GPT-2 almost never generates an \"<|endoftext|>\" token, the punchlines go on until they are truncated by our 60-token limit on Setup + Punchline.  Without that limit, they would probably just keep going...\n",
    "    \n",
    "Our fine-tuned model (green histogram above) does better, as expected from the fact that it generates \"<|endoftext|>\" tokens about half the time.  It does succeed in generating a substantial number of succinct punchlines.  However, it still looks nothing like the distribution of punchline lengths in human-generated punchlines---even when it generates an \"<|endoftext|>\" token, it still goes on too long."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d398ce3e-ef26-46be-9237-f4a7b8284230",
   "metadata": {},
   "source": [
    "<div style=background-color:#EEEEFF>\n",
    "\n",
    "### 3. How often are the punchline \"answers\" just more questions?\n",
    "    \n",
    "Sometimes real joke punchlines contain questions, for example:\n",
    "\n",
    "- Question: How many introverts does it take to change a lightbulb?\n",
    "- Answer: Why does it have to be a group activity?\n",
    "\n",
    "How frequently does it happen in real jokes, as compared to the AI-generated ones?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d8718d-b5d7-4577-bb1e-1f38c806cb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tally number of times the punchline contains \"?\"\n",
    "n_question_gpt2 = [clean_qa(x,eot)[1].count('?') for x in generated_gpt2]\n",
    "n_question_10e = [clean_qa(x,eot)[1].count('?') for x in generated_10e]\n",
    "n_question_true = [clean_qa(x,eot)[1].count('?') for x in test['full_qa'].iloc[:1000]]\n",
    "\n",
    "bins = np.arange(5)\n",
    "plt.hist(n_question_true[:1000],bins=bins,label='Human',\n",
    "                      linewidth=2, edgecolor='blue', hatch='\\\\\\\\\\\\\\\\',fill=False)\n",
    "plt.hist(n_question_gpt2,bins=bins,label='GPT2',color='r',\n",
    "         linewidth=2, edgecolor='r', hatch='//',fill=False)\n",
    "plt.hist(n_question_10e,bins=bins,alpha=0.5,label='FT: 10 epochs',color='g')\n",
    "plt.xlabel('# of \"?\" in punchline')\n",
    "plt.ylabel('# of occurences')\n",
    "l = plt.legend(bbox_to_anchor=(1, 1), loc='upper left', ncol=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f980308d-69a5-437f-a837-afa885951c7f",
   "metadata": {},
   "source": [
    "<div style=background-color:#EEEEFF>\n",
    "    \n",
    "Pre-trained GPT-2 tends to follow questions with more questions (presumably because this often happens in its training dataset), and it does this much more frequently than happens in actual Q/A jokes.  \n",
    "    \n",
    "Fine-tuning the model doesn't fix this problem; in fact, it seems to make it worse!  Maybe this is because it is learning from a training set that contains more questions than the broad survey of text use to train the vanilla distribution GPT-2, since *all* the jokes contain questions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f29b2e-6073-42de-a8a1-7c03f50c7dcf",
   "metadata": {},
   "source": [
    "<div style=background-color:#EEEEFF>\n",
    "\n",
    "### 4. How often does it insert an additional \"Answer:\" or \"Question:\" into the punchline?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048f6bd0-3321-42de-b6a4-cb3545bdd932",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tally # of 'Question:' and 'Answer:' strings in each generated text\n",
    "nq_gpt2 = [x.count('Question:') for x in generated_gpt2]\n",
    "nq_10e = [x.count('Question:') for x in generated_10e]\n",
    "nq_true = [x.count('Question:') for x in test['full_qa'].iloc[:1000]]\n",
    "\n",
    "na_gpt2 = [x.count('Answer:') for x in generated_gpt2]\n",
    "na_10e = [x.count('Answer:') for x in generated_10e]\n",
    "na_true = [x.count('Answer:') for x in test['full_qa'].iloc[:1000]]\n",
    "\n",
    "print('Of the 1,000 punchlines, how often were there extra \"Question:\" and \"Answer:\" strings?')\n",
    "print('       Human: {:>3} extra \"Question:\" jokes and {:>3} extra \"Answer:\" jokes'.format(np.sum(nq_true)-1000,\n",
    "                                                                                            np.sum(na_true)-1000))\n",
    "print('       GPT-2: {:>3} extra \"Question:\" jokes and {:>3} extra \"Answer:\" jokes'.format(np.sum(nq_gpt2)-1000,\n",
    "                                                                                            np.sum(na_gpt2)-1000))\n",
    "print('  Fine-tuned: {:>3} extra \"Question:\" jokes and {:>3} extra \"Answer:\" jokes'.format(np.sum(nq_10e)-1000,\n",
    "                                                                                            np.sum(na_10e)-1000))\n",
    "\n",
    "\n",
    "# Make histograms showing how often \"Question:\" and \"Answer:\" appear\n",
    "fig,axs = plt.subplots(1, 2, figsize=(12,4))\n",
    "plt.rcParams['font.size'] = '14'\n",
    "bins = [0.5+x for x in range(5)]\n",
    "\n",
    "axs[0].hist(nq_true[:1000],bins=bins,label='Human',\n",
    "                linewidth=2, edgecolor='blue', hatch='\\\\\\\\\\\\\\\\',fill=False)\n",
    "axs[0].hist(nq_gpt2,bins=bins,label='GPT2',\n",
    "                linewidth=2, edgecolor='r', hatch='//',fill=False)\n",
    "axs[0].hist(nq_10e,bins=bins,color='g',alpha=0.5,label='FT: 10 epochs')\n",
    "axs[0].set_xlabel('# of times \"Question:\" appears')\n",
    "axs[0].set_ylabel('# of occurences')\n",
    "l = axs[0].legend()\n",
    "\n",
    "axs[1].hist(na_true[:1000],bins=bins,label='Human',\n",
    "                linewidth=2, edgecolor='blue', hatch='\\\\\\\\\\\\\\\\',fill=False)\n",
    "axs[1].hist(na_gpt2,bins=bins,label='GPT2',\n",
    "                linewidth=2, edgecolor='r', hatch='//',fill=False)\n",
    "axs[1].hist(na_10e,bins=bins,color='g',alpha=0.5,label='FT: 10 epochs')\n",
    "axs[1].yaxis.set_label_position(\"right\")\n",
    "axs[1].yaxis.tick_right()\n",
    "axs[1].set_xlabel('# of times \"Answer:\" appears')\n",
    "axs[1].set_ylabel('# of occurences')\n",
    "l = axs[1].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951bb9f8-68e3-443a-a95f-65bbbd2d6b67",
   "metadata": {},
   "source": [
    "<div style=background-color:#EEEEFF>\n",
    "\n",
    "Very occasionally, the human punchlines have multiple answers or multiple questions, although the formatting for that can vary.  \n",
    "    \n",
    "However, GPT-2, in its effort to generate text that \"continues\" the text prompt, will often repeat the \"Question:\" or \"Answer:\" part of the prompt.  This seems to be similar to GPT-2's tendency to answer questions with more questions; it is parroting the format of the prompt it has been given, even if that is not appropriate in the context of the joke.\n",
    "    \n",
    "As we saw with the frequency of \"?\", fine-tuning seems to make this problem worse."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b56925-5bc8-4566-8b2e-1101214f6acc",
   "metadata": {},
   "source": [
    "<div style=background-color:#EEEEFF>\n",
    "\n",
    "### 5. How often can the generator fool our BERT-based joke classifier into thinking it's a human-generated joke?\n",
    "    \n",
    "Let's run the GPT-2 generated jokes (setup + punchline) through the Joke Classifier we trained in [3.PunchlineClassifier](3.PunchlineClassifier.ipynb) and see how well our fine-tuned model does.\n",
    "    \n",
    "To do that, we need to load the BERT tokenizer and the classifier we trained in [3.PunchlineClassifier](3.PunchlineClassifier.ipynb).\n",
    "    \n",
    "Then we need to pass our generated data through the BERT tokenizer and into the classifier to get predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97127fc3-4ac7-48c0-9c1c-ad7db381761c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from datasets import Dataset \n",
    "\n",
    "# Load the trained classifier model and tokenizer\n",
    "classifier_filename = 'models/ClassifyJokes_bert.pt'\n",
    "\n",
    "# Load the vanilla BERT model, plus its tokenizer\n",
    "class_checkpoint = mtools.load_checkpoint('bert')\n",
    "class_tokenizer = mtools.load_tokenizer(class_checkpoint)\n",
    "# Load our trained classifier and put it onto the GPU\n",
    "class_model = torch.load(classifier_filename, map_location=torch.device('cpu'))\n",
    "class_model.to(mtools.get_device(use_gpu=True))\n",
    "\n",
    "# Tokenize the jokes\n",
    "def class_tokenize_function(example):\n",
    "    q = [x[:x.find('Answer:')].strip() for x in example['text']]\n",
    "    a = [x[x.find('Answer:'):].strip() for x in example['text']]\n",
    "    return class_tokenizer(q, a, padding=\"max_length\", max_length=60, truncation=True)\n",
    "\n",
    "# Get predictions as to which jokes are \"real\"\n",
    "def get_predictions(input_texts):\n",
    "    texts = [x.replace(eot,'').replace('\\n',' ').strip() for x in input_texts]\n",
    "    text_dataset = Dataset.from_dict({'text': texts})\n",
    "    text_tokenized = text_dataset.map(class_tokenize_function, batched=True)\n",
    "    text_tokenized = text_tokenized.remove_columns(['text'])\n",
    "    text_tokenized.set_format('torch')\n",
    "    # Use the classifier to get predictions (1 = real joke, 0 = fake joke) \n",
    "    #     and probability of being a \"real\" joke (from 0.00 to 1.00)\n",
    "    preds = mtools.classify_punchlines(text_tokenized, class_model, \n",
    "                                       return_prob=False, use_gpu=True,\n",
    "                                       quiet=True)\n",
    "    return preds\n",
    "\n",
    "# Apply tokenization and classification to each joke dataset\n",
    "preds_gpt2 = get_predictions(generated_gpt2)\n",
    "preds_ft = get_predictions(generated_10e)\n",
    "preds_human = get_predictions(test['full_qa'].iloc[:1000])\n",
    "\n",
    "print('Of the 1,000 jokes we checked:')\n",
    "print('{:>5} of the Human punchlines get classified as \"real\" punchlines'.format(np.sum(preds_human)))\n",
    "print('{:>5} of the GPT-2 punchlines get classified as \"real\" punchlines'.format(np.sum(preds_gpt2)))\n",
    "print('{:>5} of the Fine-tuned punchlines get classified as \"real\" punchlines'.format(np.sum(preds_ft)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ae1e62-b308-4f74-b248-dc54451cf522",
   "metadata": {},
   "source": [
    "<div style=background-color:#EEEEFF>\n",
    "\n",
    "In our runs, almost all (>95%) of the human-generated jokes are recognized as being \"real\" jokes.  In contrast, almost none of the jokes generated by out-of-the-box GPT-2 can convince the classifier they are \"real\".  \n",
    "    \n",
    "The fine-tuned model does better, convincing the classifier that it has produced a \"real\" joke about 35-40% of the time.\n",
    "    \n",
    "As we saw before, both out-of-the-box GPT-2 and our Fine-tuned model have a tendency to repeat the \"Question:\" and/or \"Answer:\" format.  If we \"cheat\" and clean up the generated output by truncating anything beyond a superfluous \"Question:\" or \"Answer:\" string, how much better do we do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31345f2-b77d-4bc1-b132-3d2e490794fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_extras(text):\n",
    "    text = text.replace('\\n','')\n",
    "    while text.count('Question:') > 1:\n",
    "        text = text[:text.rfind('Question:')]\n",
    "    while text.count('Answer:') > 1:\n",
    "        text = text[:text.rfind('Answer:')]\n",
    "    return text\n",
    "\n",
    "preds_clean_gpt2 = get_predictions([strip_extras(x) for x in generated_gpt2])\n",
    "preds_clean_ft = get_predictions([strip_extras(x) for x in generated_10e])\n",
    "\n",
    "print('Of the 1,000 jokes we checked:')\n",
    "print('{:>5} of the *cleaned* GPT-2 punchlines get classified as \"real\" punchlines'.format(np.sum(preds_clean_gpt2)))\n",
    "print('{:>5} of the *cleaned* Fine-tuned punchlines get classified as \"real\" punchlines'.format(np.sum(preds_clean_ft)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72c5e66-8a81-4a7e-92ef-b8b3f7501e68",
   "metadata": {},
   "source": [
    "<div style=background-color:#EEEEFF>\n",
    "\n",
    "Even with cleaned-up output, the vanilla GPT-2 generator still fools the classifier less than 10% of the time, but now the fine-tuned model can fool it 65-70% of the time!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209ad771-e2e3-43e6-aa31-94e7030d3c84",
   "metadata": {},
   "source": [
    "<div style=background-color:#EEEEFF>\n",
    "    \n",
    "With everything we've learned here, let's make an AI joke-telling machine. \n",
    "    \n",
    "We'll use our fine-tuned model to generate candidate punchlines, and we'll do the simple post-processing step of stripping the punchline of any additional \"Question: XX\" / \"Answer: YY\" text.  Finally, we'll pass the candidate punchline through our Joke Classifier to see if it looks like a \"real\" joke.  If not, we'll get another candidate and try again.\n",
    "    \n",
    "Try experimenting with running the same setup multiple times, and/or by supplying a different joke setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7a42dd-5483-4785-8637-bc12838adf8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "setup = \"How many nerds does it take to screw in a lightbulb?\"\n",
    "\n",
    "# Format as Q/A joke\n",
    "prompt = 'Question: ' + setup + ' Answer:'\n",
    "\n",
    "# Keep generating punchlines until one convinces the classifier it's real\n",
    "joke_class = 0; counter = 0\n",
    "while joke_class==0:\n",
    "    counter += 1\n",
    "    ai_joke = strip_extras(mtools.generate(model_10e, tokenizer, prompt, quiet=True))\n",
    "    joke_class = get_predictions([ai_joke])[0]\n",
    "\n",
    "final_joke = ai_joke.replace(eot,'').strip()\n",
    "\n",
    "print('=======================================================')\n",
    "print('Here is our final, AI-generated, AI-approved joke! (N_tries = {})'.format(counter))\n",
    "print()\n",
    "print('  {}'.format(final_joke[:final_joke.find('Answer:')]))\n",
    "print('    {}'.format(final_joke[final_joke.find('Answer:'):]))\n",
    "print('=======================================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161a60f2-9d89-4395-9f3f-63f3ccaa644a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'What do you call a boat full of dentists?'\n",
    "mtools.generate(model_10e, tokenizer, prompt, quiet=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

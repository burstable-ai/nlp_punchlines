{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf2cad74-b61f-4470-98d3-feb1500b3db4",
   "metadata": {},
   "source": [
    "<div style=background-color:#EEEEFF>\n",
    "\n",
    "# Can you teach an AI to tell jokes?\n",
    "\n",
    "In this tutorial we will explore what happens when we get an AI to tell jokes using Transformer models and PyTorch, how much we can improve the performance with fine-tuning, and some of the major limitations we encounter along the way.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f95ca7d-b081-4ed8-bc87-bbdd45812d22",
   "metadata": {},
   "source": [
    "<div style=background-color:#EEEEFF>\n",
    "\n",
    "This tutorial will walk through the following:\n",
    "\n",
    "* [1.JokesDataset](1.JokesDataset.ipynb): Create a dataset of jokes to use for experimentation and model training\n",
    "* [2.FakePunchlines](2.FakePunchlines.ipynb): Use a pretrained Transformer model to generate punchlines\n",
    "* [3.PunchlineClassifier](3.PunchlineClassifier.ipynb): Train a \"joke classifier\" to tell the difference between \"real\" human-generated punchlines and \"fake\" Transformer-generated punchlines\n",
    "* [4.FineTune](4.FineTune.ipynb): Use our jokes dataset to fine-tune the Transformer models and improve the punchlines they generate\n",
    "* [5.Performance](5.Performance.ipynb): Use our joke classifier to quantify the improved performance we got from fine-tuning\n",
    "    \n",
    "Each step has its own Jupyter Notebook, so you can walk through the details of each processing step if you desire.  This notebook walks through a high-level view of the problem, with each step implemented as a single command.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc593eb0-9642-42ee-94f0-6b1ac4bb3850",
   "metadata": {},
   "source": [
    "<div style=background-color:#EEEEFF>\n",
    "\n",
    "NOTE: Some commands make take several hours to run&mdash;these commands are better run from the command line using `screen` or `tmux`, so that they can be run as a detached background process that does not require a persistent internet connection.  You can get a terminal window by selecting **File&rarr;New&rarr;Terminal**.  You can find documentation for `screen` [here](https://linuxize.com/post/how-to-use-linux-screen/) and for `tmux` [here](https://linuxize.com/post/getting-started-with-tmux/).  Both are already installed on Cloudburst."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40964901-780f-40e7-807a-fab829c5d06b",
   "metadata": {},
   "source": [
    "<div style=background-color:#EEEEFF>\n",
    "\n",
    "## 1. The Jokes Dataset\n",
    "    \n",
    "For this project, we'll use a large, publicly available dataset of jokes: the \"One Million Reddit Jokes\" dataset, which covers jokes from the /r/jokes subreddit from April 1, 2020 and earlier. The jokes dataset is provided here in `./data/one-million-reddit-jokes.csv`. You can also download the jokes dataset directly from Kaggle [here](https://www.kaggle.com/pavellexyr/one-million-reddit-jokes)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f30b9c-894b-4162-8f50-e51430665413",
   "metadata": {},
   "source": [
    "<div style=background-color:#EEEEFF>\n",
    "\n",
    "Jokes can take many forms; some are just narrative stories with amusing or suprising endings. Some are \"My life\" (as in, \"my life is a joke\", which appears 9 separate times in this dataset).\n",
    "    \n",
    "For this project, we'll restrict ourselves to a narrow, semi-formulaic set of jokes that include a \"setup\" question, followed by a \"punchline\" answer, and we'll require the punchlines to be rather short: 20 words or less.  \n",
    "    \n",
    "We also want to clean up the data to make sure we have a set of \"real\" jokes, removing jokes that have subsequently been deleted or removed and whose punchlines are therefore missing, and those that did not get a single upvote.  Finally, we want to remove duplicate jokes, while summing the upvotes across every instance of a unique joke.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4d5a18-04b6-4a1e-8f71-05a00eff67fe",
   "metadata": {},
   "source": [
    "<div style=background-color:#EEEEFF>\n",
    "\n",
    "This leaves us with a final dataset of ~150,000 short \"Q/A\" format jokes.\n",
    "    \n",
    "We will split this dataset into \"train\" and \"test\" sets, and also copy a small subset of the \"test\" jokes into a \"minitest\" dataset that we can use for development purposes.\n",
    "    \n",
    "If your interested in the details of *why* we made these choices, or how we do the data cleaning, see [1_JokesDataset.ipynb](1_JokesDataset.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b07478-66fe-47b1-ab26-87ef1166a3c6",
   "metadata": {},
   "source": [
    "<div style=background-color:#EEEEFF>\n",
    "\n",
    "You can run this functionality here in the Notebook by executing the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4cf0c2-722d-433a-b45c-28b027d452cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from clean_data import clean_data\n",
    "clean_data(file='data/one-million-reddit-jokes.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600c512e-62c3-434f-b95d-a83aa7a59bb0",
   "metadata": {},
   "source": [
    "<div style=background-color:#EEEEFF>\n",
    "\n",
    "Alternatively, you can run the cleaning script in the terminal from the command line with:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "be605fea-0d70-4e0d-8ccb-412ae1d92765",
   "metadata": {},
   "source": [
    "    prompt$> python clean_data.py data/one-million-reddit-jokes.csv "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06952ab7-401f-4ccc-a9e4-9214d4089b6a",
   "metadata": {},
   "source": [
    "<div style=background-color:#EEEEFF>\n",
    "\n",
    "## 2. \"Fake\" Punchlines, generated by an AI\n",
    "\n",
    "Before we try to train an AI to tell jokes, we should check what can be done out-of-the-box with existing, pre-trained Transformer models.  We'll use GPT-2, which is the freely-available forerunner of the recent GPT-3 text-generation model that generated tons of press last year. \n",
    "    \n",
    "GPT-2 and GPT-3 are both *auto-regressive* models, which use only the \"decoder\" part of the Transformer neuron architecture and are optimized for generating text.  They are built to take a text prompt and generate additional new text that \"continues\" the thread.\n",
    "\n",
    "Given a joke setup, can GPT-2 produce a plausible punchline? And is it funny?\n",
    "    \n",
    "A more detailed walk-through of this step can be found in [2_FakePunchlines.ipynb](2_FakePunchlines.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47ea357-51b1-4b2b-baab-2c663730f950",
   "metadata": {},
   "source": [
    "<div style=background-color:#EEEEFF>\n",
    "\n",
    "Because GPT-2 is trained to be general-purpose text generator, and not necessarily to answer questions or provide punchlines, we give it a few in-text clues to help it recognize the Q/A joke format we are trying to produce, so that each joke is formatted as:\n",
    "    \n",
    "> \"Question: [joke setup, ends in '?'] Answer: [joke punchline]\"\n",
    "    \n",
    "We then load the GPT-2 model and its associated tokenizer, which will convert text strings into numeric input for the model.  We pass it a \"prompt\", in the form \n",
    "    \n",
    "> \"Question: [joke setup] Answer:\" \n",
    "    \n",
    "and ask it to generate the continuing text that should come after \"Answer:\".  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ad8462-a452-4105-b296-5d3944c28780",
   "metadata": {},
   "source": [
    "<div style=background-color:#EEEEFF>\n",
    "\n",
    "The code checks to see if a GPU is available, and to use it if it is.  The GPU gets a 6.5x speedup over the CPU in our tests, but that still means it will take hours to generate fake punchlines for our full dataset of 140,000+ jokes.\n",
    "\n",
    "Let's do a small run with our \"minitest\" dataset and take a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c294be73-fa4f-464e-92ef-a55bbe660225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This runs locally in the Notebook on our small 300-joke \"mini-test\" set.  \n",
    "\n",
    "from fake_punchlines import add_fake_punchlines\n",
    "\n",
    "add_fake_punchlines('data/short_jokes_minitest.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6de3dd7-c867-4f90-9028-ec6ab0e66e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at exampes of the real jokes and the fake punchlines we just created\n",
    "import pandas as pd\n",
    "mini_jokes = pd.read_csv('data/short_jokes_minitest.csv')\n",
    "mini_fakes = pd.read_csv('data/short_jokes_minitest_fake.csv')\n",
    "for i in range(3):\n",
    "    print('Question: {}'.format(mini_jokes.iloc[i]['setup']))\n",
    "    print('  Answer: {}'.format(mini_jokes.iloc[i]['punchline']))\n",
    "    print('    Fake: {}'.format(mini_fakes.iloc[i]['punchline']))\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5eb029-3afa-4030-b987-9d08bedbab9d",
   "metadata": {},
   "source": [
    "<div style=background-color:#EEEEFF>\n",
    "    \n",
    "There are a few interesting things to note here.\n",
    "\n",
    "* Responses are generally on-topic and sound (mostly) like coherent English.  This is what GPT-2 is good at!\n",
    "* The responses just ramble on and cut off arbitrarily.  We set a 30-token limit if no end-of-string (EOS) token is received; an EOS token is basically *never* generated.  GPT-2 is not good at knowing when to shut up!\n",
    "* GPT-2 often answers questions with more questions (although structuring our prompts with explicit \"Question:/Answer:\" format seems to have helped a lot compared to my previous tests...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12129770-47d1-4192-996a-d4e33378536e",
   "metadata": {},
   "source": [
    "<div style=background-color:#EEEEFF>\n",
    "    \n",
    "It takes about 30 seconds to generate punchlines for a batch of 100 jokes (your mileage may vary somewhat).  \n",
    "    \n",
    "In the next step, we'll train a joke classifier to distinguish between real and fake jokes, but to do that, we'll need to generate a big dataset of fake punchlines.  Based on our small batch example, generating fake punchlines for the full dataset will take almost 12 hours!\n",
    "    \n",
    "If you have a stable internet connection and can leave your laptop open, you can run them right here in the notebook and hope that you don't get disconnected.\n",
    "\n",
    "However, a better choice is to run them from the terminal, using screen or tmux to background the process. That way, you launch the run, close your laptop, and walk away. The process will run overnight and the output will be waiting for you when you get up in the morning.\n",
    "\n",
    "To run in the background:\n",
    "* Open a terminal\n",
    "* Enter `screen`\n",
    "* Run the following command:\n",
    "```\n",
    "    python fake_punchlines.py data/short_jokes_train.csv\n",
    "```\n",
    "* Detach from the `screen`.\n",
    "    \n",
    "Then do the same for `data/short_jokes_test.csv`.\n",
    "    \n",
    "More detailed instructions on how to do this can be found in the [FakePunchlines Notebook](2_FakePunchlines.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbb9f90-22b1-491e-9ea7-c9911a9d0397",
   "metadata": {},
   "source": [
    "<div style=background-color:#EEEEFF>\n",
    "\n",
    "## 3. A Classifier to recognize \"real\" and \"fake\" punchlines\n",
    "    \n",
    "We've seen that GPT-2 straight out of the box has some trouble telling jokes.  For one thing, she tends to ramble on and on, much longer than typical Q/A joke punchlines.  She also often answers questions with questions, much more so than actual punchlines do.  If the jokes make sense at all, they usually aren't very funny.  \n",
    "    \n",
    "In short, GPT-2 can't fool a human into thinking that she's a real, human comedian.\n",
    "    \n",
    "But can GPT-2 fool another AI?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf122a2-713c-4242-b629-7da69b50a314",
   "metadata": {},
   "source": [
    "<div style=background-color:#EEEEFF>\n",
    "\n",
    "Let's find out, by training a classifier to distinguish \"real\" from \"fake\" jokes.  \n",
    "    \n",
    "For this exercies, we'll use a different type of Transformer model: an *auto-encoding* model.  Models of this type use only the \"encoder\" part of the Transformer neuron architecture, and are optimized for making sense of a text-string (including classifying it).  The particular auto-encoder we use here is called BERT.  \n",
    "    \n",
    "Can GPT-2 fool BERT into thinking her jokes are real?  Or will BERT be able to tell the difference between her jokes and those from a human(?) comedian on Reddit?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb7bb81-223d-4360-b818-33e810e3298d",
   "metadata": {},
   "source": [
    "<div style=background-color:#EEEEFF>\n",
    "\n",
    "The process of training and testing BERT to distinguish between real and fake jokes is implemented here in the *classify_punchines()* function in [punchline_classifier.py](punchline_classifier.py).  \n",
    "    \n",
    "We need to give him a training set of jokes, including both real jokes and fake jokes, for him to use to train the classifier.  We also give him a test set of jokes, including both real and fake, so that he can quantify how well he is able to do.\n",
    "    \n",
    "It takes several hours to train BERT on the full dataset, so we've made it easy to \"downsample\" the dataset by a large factor (e.g., 20x).  Even using this small subset of the training data, BERT is able to achieve good quality differentiation between the real and fake jokes.  Training on the full dataset only improves the results by a small amount.  \n",
    "    \n",
    "If you want to train on the full dataset, we recommend doing it from the command line using *screen*, as explained in [3.PunchlineClassifier](3.PunchlineClassifier.ipynb).  \n",
    "    \n",
    "For now, let's just train on 1/20th of the data, right here in the notebook (should take < 10 minutes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791599fe-3e2f-43c3-ac56-51dc1a51a20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from punchline_classifier import train_punchline_classifier\n",
    "\n",
    "train_files = ['data/short_jokes_train.csv','data/short_jokes_train_fake.csv']\n",
    "test_files = ['data/short_jokes_test.csv','data/short_jokes_test_fake.csv']\n",
    "\n",
    "# Set downsample=1 or leave out to train on the full training set (it defaults to 1)\n",
    "model = train_punchline_classifier(train_files, test_files, downsample=20)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27efce87-61fe-41f0-b6e3-dcb6a7c6b4ad",
   "metadata": {},
   "source": [
    "<div style=background-color:#EEEEFF>\n",
    "\n",
    "Notice that BERT is able to achieve 97%+ accuracy, even just training on a small subset of the available training data.\n",
    "    \n",
    "So the answer is \"No\", out-of-the-box GPT-2 cannot fool BERT with her joke-telling abilities.\n",
    "    \n",
    "But what if we train her *specifically to tell jokes*?  Can she get better at it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5964ec-ac69-43ce-b985-601aa5a2865f",
   "metadata": {},
   "source": [
    "<div style=background-color:#EEEEFF>\n",
    "\n",
    "## 4. Fine-Tuning a Text Generator \n",
    "    \n",
    "Next, we'll use our training set of short Q/A-style jokes to do some \"fine-tuning\" on the GPT-2 generator model.  This process makes use of the pre-trained GPT-2 ability to generate realistic English language text, but then trains a few more neural network layers to specifically generate the kind of text we're looking for.  \n",
    "    \n",
    "This fine-tuning model training is implemented in *fine_tune.py*.  As an example, we'll run it on 1/100th of our joke training set and train for 3 epochs, just to get the code working quickly.  \n",
    "\n",
    "If you are interested, you can see more details about setting up the dataset and models for this round of training in [4.FineTune](4.FineTune.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22e8450-9176-44e2-9db0-c341af87b1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fine_tune import fine_tune\n",
    "\n",
    "fine_tune(train_files='data/short_jokes_train.csv',\n",
    "          use_model='gpt2', downsample=100, nepochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1d17f7-4b71-422e-b799-68a83301ee11",
   "metadata": {},
   "source": [
    "<div style=background-color:#EEEEFF>\n",
    "\n",
    "We recommend training on the full dataset for about 10 epochs, which should be done in *screen* from the command line:\n",
    "    \n",
    "* `$> screen -S train_generator`\n",
    "* `$> python fine_tune.py --train data/short_jokes_train.csv --nepochs=10`\n",
    "\n",
    "Then \"Ctl-a d\" to detach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548b7284-cced-4f45-a03b-50418d9e67b3",
   "metadata": {},
   "source": [
    "<div style=background-color:#EEEEFF>\n",
    "\n",
    "## 5. How well does our fine-tuned Joke Generator perform?\n",
    "    \n",
    "Okay, so now we've done a round of fine-tuning on the joke generator.  Let's see if it performs any better than vanilla out-of-the-box GPT-2.  We'll do this two ways (see [5.Performance](5.Performance.ipynb) for additional performance metrics and an in-depth analysis): \n",
    "    \n",
    "- By passing our AI-generated jokes to the Punchline Classifier to see if it can fool the classifier\n",
    "- By playing around with the joke-generator to see if it can make us laugh!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db9b64c-91b6-4bf1-8893-faef5a4e6870",
   "metadata": {},
   "outputs": [],
   "source": [
    "from test_generator import load_all_models, generate_punchlines, get_class_predictions\n",
    "\n",
    "# These are the models we trained.  Feel free to substitute your own when you have created some!\n",
    "generator_filename = 'models/JokeGen_gpt2_1.00subset_10epochs_2022-01-07.pt'\n",
    "classifier_filename = 'models/ClassifyJokes_bert_1.00subset_2021-12-16.pt'\n",
    "\n",
    "# Load the models\n",
    "load_all_models(generator_filename=generator_filename, classifier_filename=classifier_filename)\n",
    "# Generate punchlines using vanilla GPT-2 and our fine-tuned version\n",
    "generated_gpt2, generated_ft = generate_punchlines(mini_jokes)\n",
    "# Get predictions (1=\"real\" joke, 0=\"fake\" joke)\n",
    "p_human, p_gpt2, p_ft = get_class_predictions(mini_jokes, generated_gpt2, generated_ft)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683d7245-7295-45a3-b915-56ac7db938af",
   "metadata": {},
   "source": [
    "<div style=background-color:#EEEEFF>\n",
    "\n",
    "In our runs, almost all (>97%) of the human-generated jokes are recognized as being \"real\" jokes. In contrast, almost none of the jokes generated by out-of-the-box GPT-2 can convince the classifier they are \"real\".\n",
    "\n",
    "The fine-tuned model does better, convincing the classifier that it has produced a \"real\" joke about 35-40% of the time.\n",
    "\n",
    "It turns out that both out-of-the-box GPT-2 and our Fine-tuned model have a tendency to produce multiple \"Question:... Answer:...\" sequences in the punchline (see [5.Performance](5.Performance.ipynb) for details).  For example, vanilla GPT-2 produced this punchline in one of our runs:\n",
    "\n",
    "- Question: Did you know Google now has a platform for recording your bowel movements? \n",
    "- Answer: Google+ Question: Does the internet only allow you to search for words on the internet? Answer: Yes. Answer: The internet\n",
    "    \n",
    "This would make more sense if we truncated the punchline after its first \"Answer\", before it starts asking more questions and supplying more answers, i.e.:\n",
    "    \n",
    "- Question: Did you know Google now has a platform for recording your bowel movements? \n",
    "- Answer: Google+ \n",
    "    \n",
    "If we \"cheat\", we can clean up the generated output by truncating it before any redundant \"Question:\" or \"Answer:\" sequences, can we do a better job of fooling the classifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea795bd9-3129-4e0c-b95c-eb247dd76c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_extras(text):\n",
    "    text = text.replace('\\n','')\n",
    "    while text.count('Question:') > 1:\n",
    "        text = text[:text.rfind('Question:')]\n",
    "    while text.count('Answer:') > 1:\n",
    "        text = text[:text.rfind('Answer:')]\n",
    "    return text\n",
    "\n",
    "p_human, p_gpt2, p_ft = get_class_predictions(mini_jokes, \n",
    "                                              [strip_extras(x) for x in generated_gpt2], \n",
    "                                              [strip_extras(x) for x in generated_ft])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799a5ae7-aeb8-4909-86f9-8ae9b6bf54f5",
   "metadata": {},
   "source": [
    "<div style=background-color:#EEEEFF>\n",
    "\n",
    "That clearly helped!  Now 60-70% of the punchlines generated by the fine-tuned model can fool the classifier (while vanilla GPT-2 is still struggling at a below 10% hit rate).  \n",
    "    \n",
    "Now let's bundle up the fine-tuned generator to tell a joke.  We'll do the cleaning process on the result, and then we'll pass it through the classifier.  If the classifier thinks it's a real joke, we'll display the results.  Otherwise, we'll generate a new punchline and keep trying until we get one that can fool the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed56a81a-9495-44ca-880c-7e0ab91959d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from test_performance import tell_a_joke\n",
    "\n",
    "setup = \"How many nerds does it take to change a lightbulb?\"\n",
    "\n",
    "joke = tell_a_joke(setup)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698ffee1-40e2-487f-b2cd-ff6e55aa9abb",
   "metadata": {},
   "source": [
    "<div style=background-color:#EEEEFF>\n",
    "\n",
    "Try running the same joke setup several times!  Usually, the joke generator will come up with something that is approved by the classifier within 1-3 attempts.  \n",
    "    \n",
    "Now try some different joke setups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df48a9e-ff8c-4782-a9f2-ea7d6a70aecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "setup = \"Why did the chicken cross the road?\"\n",
    "\n",
    "joke = tell_a_joke(setup)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4db0ec-09b3-4e9e-99c9-89672508a02f",
   "metadata": {},
   "source": [
    "<div style=background-color:#EEEEFF>\n",
    "\n",
    "It isn't always funny, but at least these punchlines sound like they could be punchlines.  It's almost like your 5-year-old kid is coming up with them..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152fc943-fb04-4197-b216-76a220997d91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

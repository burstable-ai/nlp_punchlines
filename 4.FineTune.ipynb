{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a5ffb0f-114e-422f-8803-167fac4f84bb",
   "metadata": {},
   "source": [
    "<div style=background-color:#EEEEFF>\n",
    "\n",
    "## 4. Fine-tune GPT-2 to tell jokes\n",
    "\n",
    "It turns out that vanilla GPT-T doesn't tell very good jokes.  That's not very surprising, given that GPT-2 is optimized to generate a wide variety of text; most text doesn't take the form of Q/A jokes!  \n",
    "    \n",
    "We can improve GPT-2's joke-telling ability with \"fine-tuning\", by running some additional training on top of the pre-trained GPT-2 model, using a dataset of Q/A jokes.  For this, we'll use the jokes training set we used previously to train our BERT joke classifier.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ffea0b5-eab7-4e1a-a376-d6ffd871555c",
   "metadata": {},
   "source": [
    "<div style=background-color:#EEEEFF>\n",
    "\n",
    "As before, we'll load our short jokes training set.  This time, we only want to load the \"real\" jokes, not the \"fake\" jokes, because we're trying to get GPT-2 to generate punchlines that look like real punchlines.  We only load the training dataset---we won't evaluate as we train, because unlike with classification, there isn't a simple quantitative metric to assess \"is this a good joke?\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49b968a4-1d4f-48f1-b7f5-508b21d0ffaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-818532f3475b5e35\n",
      "Reusing dataset csv (/home/jupyter-genevievegraves/.cache/huggingface/datasets/csv/default-818532f3475b5e35/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf907b53e5924bc78fd52bd04f748a25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/jupyter-genevievegraves/.cache/huggingface/datasets/csv/default-818532f3475b5e35/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a/cache-d7eb08caaadb6229.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1028 rows in the train dataset.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "train_files = ['data/short_jokes_train.csv']   # Only need the \"real\" joke training data\n",
    "downsample = 100   # Let's start by using only a subset of the data\n",
    "\n",
    "dataset = load_dataset('csv', data_files={'train':train_files})\n",
    "\n",
    "# Remove any badly-formatted data and downsample, if requested\n",
    "dataset = dataset.filter(lambda ex,j: ((type(ex['setup'])==str) & (type(ex['punchline'])==str) & \n",
    "                                       (j%downsample==0)),\n",
    "                         with_indices=True)    \n",
    "print('{} rows in the train dataset'.format(dataset['train'].num_rows)+'.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01ab48a-7a7e-4902-9a1d-3f5e9e687ea0",
   "metadata": {},
   "source": [
    "<div style=background-color:#EEEEFF>\n",
    "\n",
    "We then need to load our model, using the pre-trained version as our starting checkpoint, along with its tokenizer.  \n",
    "    \n",
    "We'll parse the joke setups and punchlines the same way we did before for the BERT classifier, then pass the \"Question:/Answer:\" format jokes through the GPT-2 tokenizer.\n",
    "    \n",
    "We're doing this a little differently from how we passed the data to BERT.  Because GPT-2 is such a large model, we're going to use batch-accumulation for the model gradients.  We'll let that gradient accumulation process drive the data batching, rather than doing it ahead of time.  This means the text padding will happen at that stage, rather than here in the data-prep stage.\n",
    "    \n",
    "We reformat the data for PyTorch, and then take only the *input_ids* column to pass to the model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f790b73c-4a14-4b27-a605-e3f07b7af1bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using checkpoint \"gpt2\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n",
      "Loading cached processed dataset at /home/jupyter-genevievegraves/.cache/huggingface/datasets/csv/default-818532f3475b5e35/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a/cache-6812a04ec3e6e694.arrow\n"
     ]
    }
   ],
   "source": [
    "import data_tools as dtools\n",
    "import model_tools as mtools\n",
    "\n",
    "checkpoint, tokenizer, model = mtools.load_model('gpt2')    \n",
    "\n",
    "# Tokenize the dataset\n",
    "\n",
    "def tokenize_function(example):\n",
    "    # Reformat the jokes strings into the \"Question: XX Answer: YY\" format\n",
    "    full_qa = dtools.joke_as_qa(example['setup'], example['punchline'])\n",
    "    # Split the questions from the answers (these are our two sequences)\n",
    "    q = [x[:x.find('Answer:')].strip() for x in full_qa]\n",
    "    a = [x[x.find('Answer:'):].strip() for x in full_qa]\n",
    "    # Tokenize the sequences\n",
    "    #  - pad and truncate to make all the same length for happy PyTorch tensors\n",
    "    output = tokenizer(q, a, padding=\"max_length\", max_length=60, truncation=True)\n",
    "    # Give attention to the first pad token because we want it to learn to generate\n",
    "    #     <|endoftext|> tokens!\n",
    "    for am in output['attention_mask']:\n",
    "        if 0 in am:\n",
    "            pad_start = am.index(0)\n",
    "            am[pad_start] = 1\n",
    "    return output\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_datasets.set_format(\"torch\")    \n",
    "train_dataset = tokenized_datasets['train']['input_ids']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db4cedc-6d39-439b-80f2-22518cd6c03c",
   "metadata": {},
   "source": [
    "<div style=background-color:#EEEEFF>\n",
    "\n",
    "Finally, we pass the tokenized data to the model and do our fine-tuning training pass.  The model training loop is encapsulated in the *train_generator* function, which handles the gradient accumulation and gradient descent.  \n",
    "    \n",
    "We'll start by training for 3 epochs on our small test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a821c04-dacd-4937-a598-5b0128292a38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on cuda:0\n",
      "Training epoch 0\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1028it [00:17, 59.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 1\n",
      "tensor(9.3609, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1028it [00:17, 58.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 2\n",
      "tensor(10.8984, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1028it [00:17, 58.19it/s]\n"
     ]
    }
   ],
   "source": [
    "model = mtools.train_generator(train_dataset, model, tokenizer, epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f230a9-2d7c-4796-b306-25e1a6d9c6cf",
   "metadata": {},
   "source": [
    "<div style=background-color:#EEEEFF>\n",
    "\n",
    "Now we're ready to train the generator on the entire dataset, and over more epochs.  Running on the full dataset takes about half an hour per epoch, so we recommend running it from the command line in a detached screen, as we have done before.\n",
    "\n",
    "* `$> screen -S train_generator`\n",
    "* `$> python fine_tune.py --train data/short_jokes_train.csv --nepochs=10`\n",
    "\n",
    "Then \"Ctl-a d\" to detach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8e0148-beaf-4667-815c-4c27a94d0499",
   "metadata": {},
   "source": [
    "<div style=background-color:#EEEEFF>\n",
    "\n",
    "When the model is done training, it gets stored in the *models/* directory.  The default filename includes identifying information about the run (base model, subset fraction, number of epochs) as well as a date stamp.\n",
    "    \n",
    "In [5.Performance](5.Performance.ipynb), we'll take a look at how much better (or not) the fine-tuned generator does at joke-telling compared with the original pre-trained GPT-2 generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07f9b7b-8871-4c7a-857e-c9ccf65d7a9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

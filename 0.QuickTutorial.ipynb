{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf2cad74-b61f-4470-98d3-feb1500b3db4",
   "metadata": {},
   "source": [
    "<div style=background-color:#EEEEFF>\n",
    "\n",
    "# Can you teach an AI to tell jokes?\n",
    "\n",
    "In this tutorial we will explore what happens when we get an AI to tell jokes using Transformer models and PyTorch, how much we can improve the performance with fine-tuning, and some of the major limitations we encounter along the way.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f95ca7d-b081-4ed8-bc87-bbdd45812d22",
   "metadata": {},
   "source": [
    "<div style=background-color:#EEEEFF>\n",
    "\n",
    "This tutorial will walk through the following:\n",
    "\n",
    "* [1.JokesDataset](1.JokesDataset.ipynb): Create a dataset of jokes to use for experimentation and model training\n",
    "* [2.FakePunchlines](2.FakePunchlines.ipynb): Use a pretrained Transformer model to generate punchlines\n",
    "* [3.PunchlineClassifier](3.PunchlineClassifier.ipynb): Train a \"joke classifier\" to tell the difference between \"real\" human-generated punchlines and \"fake\" Transformer-generated punchlines\n",
    "* [4.FineTune](4.FineTune.ipynb): Use our jokes dataset to fine-tune the Transformer models and improve the punchlines they generate\n",
    "* [5.Performance](5.Performance.ipynb): Use our joke classifier to quantify the improved performance we got from fine-tuning\n",
    "    \n",
    "Each step has its own Jupyter Notebook, so you can walk through the details of each processing step if you desire.  This notebook walks through a high-level view of the problem, with each step implemented as a single command.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc593eb0-9642-42ee-94f0-6b1ac4bb3850",
   "metadata": {},
   "source": [
    "<div style=background-color:#EEEEFF>\n",
    "\n",
    "NOTE: Some commands make take several hours to run&mdash;these commands are better run from the command line using `screen` or `tmux`, so that they can be run as a detached background process that does not require a persistent internet connection.  You can get a terminal window by selecting **File&rarr;New&rarr;Terminal**.  You can find documentation for `screen` [here](https://linuxize.com/post/how-to-use-linux-screen/) and for `tmux` [here](https://linuxize.com/post/getting-started-with-tmux/).  Both are already installed on Cloudburst."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8dae729b-9d9e-4290-95c6-6c6ead36e297",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40964901-780f-40e7-807a-fab829c5d06b",
   "metadata": {},
   "source": [
    "<div style=background-color:#EEEEFF>\n",
    "\n",
    "## 1. The Jokes Dataset\n",
    "    \n",
    "For this project, we'll use a large, publicly available dataset of jokes: the \"One Million Reddit Jokes\" dataset, which covers jokes from the /r/jokes subreddit from April 1, 2020 and earlier. The jokes dataset is provided on Clodburst servers in `/opt/tljh/user/share/nlp_punchlines_data/one-million-reddit-jokes.csv`. You can also download the jokes dataset directly from Kaggle [here](https://www.kaggle.com/pavellexyr/one-million-reddit-jokes)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f30b9c-894b-4162-8f50-e51430665413",
   "metadata": {},
   "source": [
    "<div style=background-color:#EEEEFF>\n",
    "\n",
    "Jokes can take many forms; some are just narrative stories with amusing or suprising endings. Some are \"My life\" (as in, \"my life is a joke\", which appears 9 separate times in this dataset).\n",
    "    \n",
    "For this project, we'll restrict ourselves to a narrow, semi-formulaic set of jokes that include a \"setup\" question, followed by a \"punchline\" answer, and we'll require the punchlines to be rather short: 20 words or less.  \n",
    "    \n",
    "We also want to clean up the data to make sure we have a set of \"real\" jokes, removing jokes that have subsequently been deleted or removed and whose punchlines are therefore missing, and those that did not get a single upvote.  Finally, we want to remove duplicate jokes, while summing the upvotes across every instance of a unique joke.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4d5a18-04b6-4a1e-8f71-05a00eff67fe",
   "metadata": {},
   "source": [
    "<div style=background-color:#EEEEFF>\n",
    "\n",
    "This leaves us with a final dataset of ~150,000 short \"Q/A\" format jokes.\n",
    "    \n",
    "We will split this dataset into \"train\" and \"test\" sets, and also copy a small subset of the \"test\" jokes into a \"minitest\" dataset that we can use for development purposes.\n",
    "    \n",
    "If your interested in the details of *why* we made these choices, or how we do the data cleaning, see [1.JokesDataset.ipynb](1.JokesDataset.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b07478-66fe-47b1-ab26-87ef1166a3c6",
   "metadata": {},
   "source": [
    "<div style=background-color:#EEEEFF>\n",
    "\n",
    "You can run this functionality here in the Notebook by executing the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c4cf0c2-722d-433a-b45c-28b027d452cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading in raw jokes data...\n",
      "   1000000 jokes in the raw dataset\n",
      "    146796 jokes in the final dataset (Q|A format, short punchlines, 1+ upvotes)\n",
      "Joke splits written to:\n",
      "    146796 in data/short_jokes_all.csv\n",
      "    102757 in data/short_jokes_train.csv\n",
      "     44039 in data/short_jokes_test.csv\n",
      "       300 in data/short_jokes_minitest.csv\n"
     ]
    }
   ],
   "source": [
    "from clean_data import clean_data\n",
    "!mkdir -p data\n",
    "clean_data(file='/opt/tljh/user/share/nlp_punchlines_data/one-million-reddit-jokes.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600c512e-62c3-434f-b95d-a83aa7a59bb0",
   "metadata": {},
   "source": [
    "<div style=background-color:#EEEEFF>\n",
    "\n",
    "Alternatively, you can run the cleaning script in the terminal from the command line with:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "be605fea-0d70-4e0d-8ccb-412ae1d92765",
   "metadata": {},
   "source": [
    "    prompt$> python clean_data.py /opt/tljh/user/share/nlp_punchlines_data/one-million-reddit-jokes.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06952ab7-401f-4ccc-a9e4-9214d4089b6a",
   "metadata": {},
   "source": [
    "<div style=background-color:#EEEEFF>\n",
    "\n",
    "## 2. \"Fake\" Punchlines, generated by an AI\n",
    "\n",
    "Before we try to train an AI to tell jokes, we should check what can be done out-of-the-box with existing, pre-trained Transformer models.  We'll use GPT-2, which is the freely-available forerunner of the recent GPT-3 text-generation model that generated tons of press last year. \n",
    "    \n",
    "GPT-2 and GPT-3 are both *auto-regressive* models, which use only the \"decoder\" part of the Transformer neuron architecture and are optimized for generating text.  They are built to take a text prompt and generate additional new text that \"continues\" the thread.\n",
    "\n",
    "Given a joke setup, can GPT-2 produce a plausible punchline? And is it funny?\n",
    "    \n",
    "A more detailed walk-through of this step can be found in [2_FakePunchlines.ipynb](2_FakePunchlines.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47ea357-51b1-4b2b-baab-2c663730f950",
   "metadata": {},
   "source": [
    "<div style=background-color:#EEEEFF>\n",
    "\n",
    "Because GPT-2 is trained to be general-purpose text generator, and not necessarily to answer questions or provide punchlines, we give it a few in-text clues to help it recognize the Q/A joke format we are trying to produce, so that each joke is formatted as:\n",
    "    \n",
    "> \"Question: [joke setup, ends in '?'] Answer: [joke punchline]\"\n",
    "    \n",
    "We then load the GPT-2 model and its associated tokenizer, which will convert text strings into numeric input for the model.  We pass it a \"prompt\", in the form \n",
    "    \n",
    "> \"Question: [joke setup] Answer:\" \n",
    "    \n",
    "and ask it to generate the continuing text that should come after \"Answer:\".  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ad8462-a452-4105-b296-5d3944c28780",
   "metadata": {},
   "source": [
    "<div style=background-color:#EEEEFF>\n",
    "\n",
    "The code checks to see if a GPU is available, and to use it if it is.  The GPU gets a 6.5x speedup over the CPU in our tests, but that still means it will take hours to generate fake punchlines for our full dataset of 140,000+ jokes.\n",
    "\n",
    "Let's do a small run with our \"minitest\" dataset and take a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c294be73-fa4f-464e-92ef-a55bbe660225",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300 jokes in the dataset\n",
      "Using checkpoint \"gpt2\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on jokes 0:100 out of 300 -- 0:00:00.000005\n",
      "Running on cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:28<00:00,  3.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done writing 0:100 to CSV\n",
      "Working on jokes 100:200 out of 300 -- 0:00:32.389823\n",
      "Running on cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:28<00:00,  3.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done writing 100:200 to CSV\n",
      "Working on jokes 200:300 out of 300 -- 0:01:00.966961\n",
      "Running on cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:28<00:00,  3.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done writing 200:300 to CSV\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# This runs locally in the Notebook on our small 300-joke \"mini-test\" set.  \n",
    "\n",
    "from fake_punchlines import add_fake_punchlines\n",
    "\n",
    "add_fake_punchlines('data/short_jokes_minitest.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6de3dd7-c867-4f90-9028-ec6ab0e66e84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Did you know Google now has a platform for recording your bowel movements?\n",
      "  Answer: It's called Google Sheets.\n",
      "    Fake: Yes.  Question: Who did Google's information-generating technology think you were looking for? Answer: Google's software developers.\n",
      "----\n",
      "Question: What do you call a boat full of dentists?\n",
      "  Answer: A tooth ferry\n",
      "    Fake: It's what they call a \"cough.\"  There is a tendency, though, to ascribe to the cardinal rule that dentists need\n",
      "----\n",
      "Question: How do you know someone is feeling horny?\n",
      "  Answer: They click on this post\n",
      "    Fake: The person is not fully clothed or in an aroused state.  Stacy or Kendra (joint) Skin  Question: How does\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "# Take a look at exampes of the real jokes and the fake punchlines we just created\n",
    "import pandas as pd\n",
    "mini_jokes = pd.read_csv('data/short_jokes_minitest.csv')\n",
    "mini_fakes = pd.read_csv('data/short_jokes_minitest_fake.csv')\n",
    "for i in range(3):\n",
    "    print('Question: {}'.format(mini_jokes.iloc[i]['setup']))\n",
    "    print('  Answer: {}'.format(mini_jokes.iloc[i]['punchline']))\n",
    "    print('    Fake: {}'.format(mini_fakes.iloc[i]['punchline']))\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5eb029-3afa-4030-b987-9d08bedbab9d",
   "metadata": {},
   "source": [
    "<div style=background-color:#EEEEFF>\n",
    "    \n",
    "There are a few interesting things to note here.\n",
    "\n",
    "* Responses are generally on-topic and sound (mostly) like coherent English.  This is what GPT-2 is good at!\n",
    "* The responses just ramble on and cut off arbitrarily.  We set a 30-token limit if no end-of-string (EOS) token is received; an EOS token is basically *never* generated.  GPT-2 is not good at knowing when to shut up!\n",
    "* GPT-2 often answers questions with more questions (although structuring our prompts with explicit \"Question:/Answer:\" format seems to have helped a lot compared to my previous tests...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12129770-47d1-4192-996a-d4e33378536e",
   "metadata": {},
   "source": [
    "<div style=background-color:#EEEEFF>\n",
    "    \n",
    "It takes about 30 seconds to generate punchlines for a batch of 100 jokes (your mileage may vary somewhat).  \n",
    "    \n",
    "In the next step, we'll train a joke classifier to distinguish between real and fake jokes, but to do that, we'll need to generate a big dataset of fake punchlines.  Based on our small batch example, generating fake punchlines for the full dataset will take almost 12 hours!\n",
    "    \n",
    "If you have a stable internet connection and can leave your laptop open, you can run them right here in the notebook and hope that you don't get disconnected.\n",
    "\n",
    "However, a better choice is to run them from the terminal, using screen or tmux to background the process. That way, you launch the run, close your laptop, and walk away. The process will run overnight and the output will be waiting for you when you get up in the morning.\n",
    "\n",
    "To run in the background:\n",
    "* Open a terminal\n",
    "* Enter `screen`\n",
    "* Run the following command:\n",
    "```\n",
    "    prompt$> python fake_punchlines.py data/short_jokes_train.csv\n",
    "```\n",
    "* Detach from the `screen`. (ctl+a, d)\n",
    "    \n",
    "Then do the same for `data/short_jokes_test.csv`:\n",
    "```\n",
    "    prompt$> python fake_punchlines.py data/short_jokes_test.csv\n",
    "```\n",
    "More detailed instructions on how to do this can be found in the [FakePunchlines Notebook](2_FakePunchlines.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbb9f90-22b1-491e-9ea7-c9911a9d0397",
   "metadata": {},
   "source": [
    "<div style=background-color:#EEEEFF>\n",
    "\n",
    "## 3. A Classifier to recognize \"real\" and \"fake\" punchlines\n",
    "    \n",
    "We've seen that GPT-2 straight out of the box has some trouble telling jokes.  For one thing, she tends to ramble on and on, much longer than typical Q/A joke punchlines.  She also often answers questions with questions, much more so than actual punchlines do.  If the jokes make sense at all, they usually aren't very funny.  \n",
    "    \n",
    "In short, GPT-2 can't fool a human into thinking that she's a real, human comedian.\n",
    "    \n",
    "But can GPT-2 fool another AI?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf122a2-713c-4242-b629-7da69b50a314",
   "metadata": {},
   "source": [
    "<div style=background-color:#EEEEFF>\n",
    "\n",
    "Let's find out, by training a classifier to distinguish \"real\" from \"fake\" jokes.  \n",
    "    \n",
    "For this exercise, we'll use a different type of Transformer model: an *auto-encoding* model.  Models of this type use only the \"encoder\" part of the Transformer neuron architecture, and are optimized for making sense of a text-string (including classifying it).  The particular auto-encoder we use here is called BERT.  \n",
    "    \n",
    "Can GPT-2 fool BERT into thinking her jokes are real?  Or will BERT be able to tell the difference between her jokes and those from a human(?) comedian on Reddit?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb7bb81-223d-4360-b818-33e810e3298d",
   "metadata": {},
   "source": [
    "<div style=background-color:#EEEEFF>\n",
    "\n",
    "The process of training and testing BERT to distinguish between real and fake jokes is implemented here in the *classify_punchines()* function in [punchline_classifier.py](punchline_classifier.py).  \n",
    "    \n",
    "We need to give him a training set of jokes, including both real jokes and fake jokes, for him to use to train the classifier.  We also give him a test set of jokes, including both real and fake, so that he can quantify how well he is able to do.\n",
    "    \n",
    "It takes several hours to train BERT on the full dataset, so we've made it easy to \"downsample\" the dataset by a large factor (e.g., 20x).  Even using this small subset of the training data, BERT is able to achieve good quality differentiation between the real and fake jokes.  Training on the full dataset only improves the results by a small amount.  \n",
    "    \n",
    "If you want to train on the full dataset, we recommend doing it from the command line using *screen*, as explained in [3.PunchlineClassifier](3.PunchlineClassifier.ipynb).  \n",
    "    \n",
    "For now, let's just train on 1/20th of the data, right here in the notebook (should take < 10 minutes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "791599fe-3e2f-43c3-ac56-51dc1a51a20d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-1b54771baac838d3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to /home/jupyter-genevievegraves/.cache/huggingface/datasets/csv/default-1b54771baac838d3/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a5b46abed644db7a25ac30a81cfcb3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba00aaaa69cc464dbf23ecd345fa472d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to /home/jupyter-genevievegraves/.cache/huggingface/datasets/csv/default-1b54771baac838d3/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ece66d3133bf45f49c83db8e7d11f593",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c8f54f681ab4255be74d30624cedccf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/206 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4ddfafcd19b4de297436615ec68c3db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/89 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10276 rows in the train dataset (20x downsampled).\n",
      "4404 rows in the test dataset (20x downsampled).\n",
      "Using checkpoint \"bert-base-uncased\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using eos_token, but it is not set yet.\n",
      "Using bos_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: no EOS token detected.  Adding an EOS token\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac62386a228b4e3496e1fea53bc03440",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc0b864515c7491ea475b8ed51828c26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc71c379a5e54848b2d14106d1214645",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5124f11c06f246d3b7ae170d5ade06bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▋        | 628/3855 [01:08<05:56,  9.04it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3105276/2461968356.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Set downsample=1 or leave out to train on the full training set (it defaults to 1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_punchline_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_files\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_files\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownsample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/examples/nlp_punchlines/punchline_classifier.py\u001b[0m in \u001b[0;36mtrain_punchline_classifier\u001b[0;34m(train_files, test_files, downsample)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;31m# Train the model!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmtools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized_datasets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;31m# Save the trained model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/examples/nlp_punchlines/model_tools.py\u001b[0m in \u001b[0;36mtrain_classifier\u001b[0;34m(dataset, model, use_gpu, batch_size, epochs, lr, warmup_steps, output_dir, output_prefix, save_model_on_epoch)\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m             \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/tljh/user/lib/python3.8/site-packages/torch/optim/lr_scheduler.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m                 \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0mwrapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0;31m# Note that the returned function here is no longer a bound method,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/tljh/user/lib/python3.8/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/optimization.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    349\u001b[0m                 \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m                 \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"eps\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m                 \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"lr\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from punchline_classifier import train_punchline_classifier\n",
    "!mkdir -p models\n",
    "\n",
    "train_files = ['data/short_jokes_train.csv','data/short_jokes_train_fake.csv']\n",
    "test_files = ['data/short_jokes_test.csv','data/short_jokes_test_fake.csv']\n",
    "\n",
    "# Set downsample=1 or leave out to train on the full training set (it defaults to 1)\n",
    "model = train_punchline_classifier(train_files, test_files, downsample=20)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27efce87-61fe-41f0-b6e3-dcb6a7c6b4ad",
   "metadata": {},
   "source": [
    "<div style=background-color:#EEEEFF>\n",
    "\n",
    "Notice that BERT is able to achieve 97%+ accuracy, even just training on a small subset of the available training data.\n",
    "    \n",
    "So the answer is \"No\", out-of-the-box GPT-2 cannot fool BERT with her joke-telling abilities.\n",
    "    \n",
    "But what if we train her *specifically to tell jokes*?  Can she get better at it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5964ec-ac69-43ce-b985-601aa5a2865f",
   "metadata": {},
   "source": [
    "<div style=background-color:#EEEEFF>\n",
    "\n",
    "## 4. Fine-Tuning a Text Generator \n",
    "    \n",
    "Next, we'll use our training set of short Q/A-style jokes to do some \"fine-tuning\" on the GPT-2 generator model.  This process makes use of the pre-trained GPT-2 ability to generate realistic English language text, but then trains a few more neural network layers to specifically generate the kind of text we're looking for.  \n",
    "    \n",
    "This fine-tuning model training is implemented in *fine_tune.py*.  As an example, we'll run it on 1/100th of our joke training set and train for 3 epochs, just to get the code working quickly.  \n",
    "\n",
    "If you are interested, you can see more details about setting up the dataset and models for this round of training in [4.FineTune](4.FineTune.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b22e8450-9176-44e2-9db0-c341af87b1b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-2b7301f506423f66\n",
      "Reusing dataset csv (/home/jupyter-genevievegraves/.cache/huggingface/datasets/csv/default-2b7301f506423f66/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "700ee69df19d45978adc1de0d3e2c29d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/jupyter-genevievegraves/.cache/huggingface/datasets/csv/default-2b7301f506423f66/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a/cache-fd30c2679587276b.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1028 rows in the train dataset (100x downsampled).\n",
      "Using checkpoint \"gpt2\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n",
      "Loading cached processed dataset at /home/jupyter-genevievegraves/.cache/huggingface/datasets/csv/default-2b7301f506423f66/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a/cache-d9a1bf3d6936c896.arrow\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 148.00 MiB (GPU 0; 14.76 GiB total capacity; 2.99 GiB already allocated; 11.75 MiB free; 3.18 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3105276/754182392.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfine_tune\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfine_tune\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m fine_tune(train_files='data/short_jokes_train.csv',\n\u001b[0m\u001b[1;32m      4\u001b[0m           use_model='gpt2', downsample=100, nepochs=3)\n",
      "\u001b[0;32m~/examples/nlp_punchlines/fine_tune.py\u001b[0m in \u001b[0;36mfine_tune\u001b[0;34m(train_files, use_model, downsample, nepochs)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenized_datasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmtools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'models'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'models'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/examples/nlp_punchlines/model_tools.py\u001b[0m in \u001b[0;36mtrain_generator\u001b[0;34m(train_dataset, model, use_gpu, batch_size, epochs, lr, max_seq_len, warmup_steps, output_dir, output_prefix, test_mode, save_model_on_epoch)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0mtrain_dataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_gpu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Put model in \"train\" mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/examples/nlp_punchlines/model_tools.py\u001b[0m in \u001b[0;36mset_device\u001b[0;34m(model, use_gpu, quiet)\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'No GPU available!'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;31m# Put model on the requested device (default=GPU if avaiable)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mquiet\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Running on {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/tljh/user/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    850\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    851\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 852\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    853\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m     def register_backward_hook(\n",
      "\u001b[0;32m/opt/tljh/user/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    528\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/tljh/user/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    528\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/tljh/user/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    550\u001b[0m                 \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m                     \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    553\u001b[0m                 \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/tljh/user/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    848\u001b[0m                 return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,\n\u001b[1;32m    849\u001b[0m                             non_blocking, memory_format=convert_to_format)\n\u001b[0;32m--> 850\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    851\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    852\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 148.00 MiB (GPU 0; 14.76 GiB total capacity; 2.99 GiB already allocated; 11.75 MiB free; 3.18 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "from fine_tune import fine_tune\n",
    "\n",
    "fine_tune(train_files='data/short_jokes_train.csv',\n",
    "          use_model='gpt2', downsample=100, nepochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1d17f7-4b71-422e-b799-68a83301ee11",
   "metadata": {},
   "source": [
    "<div style=background-color:#EEEEFF>\n",
    "\n",
    "We recommend training on the full dataset for about 10 epochs, which should be done in *screen* from the command line:\n",
    "    \n",
    "* `$> screen -S train_generator`\n",
    "* `$> python fine_tune.py --train data/short_jokes_train.csv --nepochs=10`\n",
    "\n",
    "Then \"Ctl-a d\" to detach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548b7284-cced-4f45-a03b-50418d9e67b3",
   "metadata": {},
   "source": [
    "<div style=background-color:#EEEEFF>\n",
    "\n",
    "## 5. How well does our fine-tuned Joke Generator perform?\n",
    "    \n",
    "Okay, so now we've done a round of fine-tuning on the joke generator.  Let's see if it performs any better than vanilla out-of-the-box GPT-2.  We'll do this two ways (see [5.Performance](5.Performance.ipynb) for additional performance metrics and an in-depth analysis): \n",
    "    \n",
    "- By passing our AI-generated jokes to the Punchline Classifier to see if it can fool the classifier\n",
    "- By playing around with the joke-generator to see if it can make us laugh!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8db9b64c-91b6-4bf1-8893-faef5a4e6870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load the classifier...\n",
      "Using checkpoint \"bert-base-uncased\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using eos_token, but it is not set yet.\n",
      "Using bos_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: no EOS token detected.  Adding an EOS token\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'models/ClassifyJokes_bert_1.00subset_2021-12-16.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_9258/1945098705.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Load the models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mload_all_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator_filename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgenerator_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifier_filename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclassifier_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;31m# Generate punchlines using vanilla GPT-2 and our fine-tuned version\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mgenerated_gpt2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerated_ft\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_punchlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmini_jokes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/nlp_punchlines/test_generator.py\u001b[0m in \u001b[0;36mload_all_models\u001b[0;34m(generator_filename, classifier_filename, use_gpu)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mclassifier_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifier_tokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmtools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bert'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m# Load our trained classifier and put it onto the GPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mclassifier_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassifier_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0mclassifier_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmtools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassifier_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_gpu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    592\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 594\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    595\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'models/ClassifyJokes_bert_1.00subset_2021-12-16.pt'"
     ]
    }
   ],
   "source": [
    "from test_generator import load_all_models, generate_punchlines, get_class_predictions\n",
    "\n",
    "# These are the models we trained.  Feel free to substitute your own when you have created some!\n",
    "generator_filename = 'models/JokeGen_gpt2.pt'\n",
    "classifier_filename = 'models/ClassifyJokes_bert.pt'\n",
    "\n",
    "# Load the models\n",
    "load_all_models(generator_filename=generator_filename, classifier_filename=classifier_filename)\n",
    "# Generate punchlines using vanilla GPT-2 and our fine-tuned version\n",
    "generated_gpt2, generated_ft = generate_punchlines(mini_jokes)\n",
    "# Get predictions (1=\"real\" joke, 0=\"fake\" joke)\n",
    "p_human, p_gpt2, p_ft = get_class_predictions(mini_jokes, generated_gpt2, generated_ft)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683d7245-7295-45a3-b915-56ac7db938af",
   "metadata": {},
   "source": [
    "<div style=background-color:#EEEEFF>\n",
    "\n",
    "In our runs, almost all (>97%) of the human-generated jokes are recognized as being \"real\" jokes. In contrast, almost none of the jokes generated by out-of-the-box GPT-2 can convince the classifier they are \"real\".\n",
    "\n",
    "The fine-tuned model does better, convincing the classifier that it has produced a \"real\" joke about 35-40% of the time.\n",
    "\n",
    "It turns out that both out-of-the-box GPT-2 and our Fine-tuned model have a tendency to produce multiple \"Question:... Answer:...\" sequences in the punchline (see [5.Performance](5.Performance.ipynb) for details).  For example, vanilla GPT-2 produced this punchline in one of our runs:\n",
    "\n",
    "- Question: Did you know Google now has a platform for recording your bowel movements? \n",
    "- Answer: Google+ Question: Does the internet only allow you to search for words on the internet? Answer: Yes. Answer: The internet\n",
    "    \n",
    "This would make more sense if we truncated the punchline after its first \"Answer\", before it starts asking more questions and supplying more answers, i.e.:\n",
    "    \n",
    "- Question: Did you know Google now has a platform for recording your bowel movements? \n",
    "- Answer: Google+ \n",
    "    \n",
    "If we \"cheat\", we can clean up the generated output by truncating it before any redundant \"Question:\" or \"Answer:\" sequences, can we do a better job of fooling the classifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea795bd9-3129-4e0c-b95c-eb247dd76c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_extras(text):\n",
    "    text = text.replace('\\n','')\n",
    "    while text.count('Question:') > 1:\n",
    "        text = text[:text.rfind('Question:')]\n",
    "    while text.count('Answer:') > 1:\n",
    "        text = text[:text.rfind('Answer:')]\n",
    "    return text\n",
    "\n",
    "p_human, p_gpt2, p_ft = get_class_predictions(mini_jokes, \n",
    "                                              [strip_extras(x) for x in generated_gpt2], \n",
    "                                              [strip_extras(x) for x in generated_ft])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799a5ae7-aeb8-4909-86f9-8ae9b6bf54f5",
   "metadata": {},
   "source": [
    "<div style=background-color:#EEEEFF>\n",
    "\n",
    "That clearly helped!  Now 60-70% of the punchlines generated by the fine-tuned model can fool the classifier (while vanilla GPT-2 is still struggling at a below 10% hit rate).  \n",
    "    \n",
    "Now let's bundle up the fine-tuned generator to tell a joke.  We'll do the cleaning process on the result, and then we'll pass it through the classifier.  If the classifier thinks it's a real joke, we'll display the results.  Otherwise, we'll generate a new punchline and keep trying until we get one that can fool the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed56a81a-9495-44ca-880c-7e0ab91959d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from test_performance import tell_a_joke\n",
    "\n",
    "setup = \"How many nerds does it take to change a lightbulb?\"\n",
    "\n",
    "joke = tell_a_joke(setup)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698ffee1-40e2-487f-b2cd-ff6e55aa9abb",
   "metadata": {},
   "source": [
    "<div style=background-color:#EEEEFF>\n",
    "\n",
    "Try running the same joke setup several times!  Usually, the joke generator will come up with something that is approved by the classifier within 1-3 attempts.  \n",
    "    \n",
    "Now try some different joke setups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df48a9e-ff8c-4782-a9f2-ea7d6a70aecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "setup = \"Why did the chicken cross the road?\"\n",
    "\n",
    "joke = tell_a_joke(setup)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4db0ec-09b3-4e9e-99c9-89672508a02f",
   "metadata": {},
   "source": [
    "<div style=background-color:#EEEEFF>\n",
    "\n",
    "It isn't always funny, but at least these punchlines sound like they could be punchlines.  It's almost like your 5-year-old kid is coming up with them..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152fc943-fb04-4197-b216-76a220997d91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

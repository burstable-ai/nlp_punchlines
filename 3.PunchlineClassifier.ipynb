{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2860c465-b905-4230-a182-e45f49a5621b",
   "metadata": {},
   "source": [
    "<div style=background-color:#EEEEFF>\n",
    "\n",
    "## 3. Recognizing \"Real\" vs. \"Fake\" jokes\n",
    "\n",
    "In [2.FakePunchlines](2.FakePunchlines.ipynb), we got GPT-2 to generate the punchlines to some jokes.  A human can pretty easily tell which are the real punchlines and which are the GPT-2-generated ones.  But can GPT-2 fool another AI?\n",
    "    \n",
    "In this Notebook, we'll use the HugginFace [transformers](https://github.com/huggingface/transformers) library to train an NLP classifier to distinguish between the real joke punchlines and the fake ones generated by GPT-2.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3474abf-3593-47bf-bef6-7dddae41db88",
   "metadata": {},
   "source": [
    "<div style=background-color:#EEEEFF>\n",
    "\n",
    "We start by loading in our training and test datasets that were generated in [1.JokesDataset](1.JokesDataset.ipynb).  While we're trying to get things to work, let's downsample the data by 100x (i.e., use only 1% of the data), just so that things will run fast.  When we're ready to train for real, we'll use a factor of 1x (no downsampling)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4be1b1dc-117a-4213-bf32-636f913d51b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-22e578e895d47822\n",
      "Reusing dataset csv (/home/jupyter-genevievegraves/.cache/huggingface/datasets/csv/default-22e578e895d47822/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26d0ac9c000248c0b92dc8fdc1108808",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/jupyter-genevievegraves/.cache/huggingface/datasets/csv/default-22e578e895d47822/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a/cache-ebffb6372e26d996.arrow\n",
      "Loading cached processed dataset at /home/jupyter-genevievegraves/.cache/huggingface/datasets/csv/default-22e578e895d47822/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a/cache-e67abdb78f2c3222.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2056 rows in the train dataset (100x downsampled).\n",
      "881 rows in the test dataset (100x downsampled).\n"
     ]
    }
   ],
   "source": [
    "# Load our dataset of real and fake jokes, split into training and test sets\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "# For development purposes, we downsample by 100x, just so things run fast\n",
    "downsample_factor = 100\n",
    "# Load the data, filter out any non-string input (it breaks the code), and downsample\n",
    "dataset = load_dataset('csv', data_files={'train':['data/short_jokes_train.csv','data/short_jokes_train_fake.csv'],\n",
    "                                          'test':['data/short_jokes_test.csv','data/short_jokes_test_fake.csv']})\n",
    "dataset = dataset.filter(lambda ex,j: ((type(ex['setup'])==str) & (type(ex['punchline'])==str) & \n",
    "                                       (j%downsample_factor==0)),                         \n",
    "                         with_indices=True)\n",
    "print('{} rows in the train dataset ({}x downsampled).'.format(dataset['train'].num_rows,downsample_factor))\n",
    "print('{} rows in the test dataset ({}x downsampled).'.format(dataset['test'].num_rows,downsample_factor))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71388c8f-d022-4aca-a001-3f8061ff4c9e",
   "metadata": {},
   "source": [
    "<div style=background-color:#EEEEFF>\n",
    "\n",
    "We've loaded the data as a HuggingFace \"dataset\", which can be read incrementally from disk (instead of loading the whole dataset into memory at once), and which can be fed nicely into PyTorch DataLoaders when it comes time to train a model.  \n",
    "\n",
    "Notice that we've mixed together the \"real\" jokes and the \"fake\" jokes (whose punchlines are generated by GPT-2) in both our \"train\" and \"test\" datasets.  The difference is that all the \"real\" jokes have score > 0 (because we only selected jokes with at least one upvote), whereas we created all the fake jokes with score = 0.\n",
    "   \n",
    "Let's take a quick look at how the data are formatted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f16cee6-06f8-40a6-be97-7cb819f1369e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['setup', 'punchline', 'score'],\n",
      "        num_rows: 2056\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['setup', 'punchline', 'score'],\n",
      "        num_rows: 881\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1d66d7-7de1-4988-a722-ea9c893ea0f0",
   "metadata": {},
   "source": [
    "<div style=background-color:#EEEEFF>\n",
    "\n",
    "The dataset is stored as a DatasetDict, which has two components, \"train\" and \"test\", each of which contain the data rows and the \"features\" from the CSV.\n",
    "    \n",
    "We can look at the first examples of the \"real\" and the \"fake\" jokes in our test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9521a00c-444c-48b4-906b-4f1a30f79f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A joke with a human-generated punchline:\n",
      "{'setup': 'Did you know Google now has a platform for recording your bowel movements?', 'punchline': \"It's called Google Sheets.\", 'score': 9}\n",
      "\n",
      "A joke with a GPT2-generated punchline:\n",
      "{'setup': 'What do you get when you cross a sheep and a kangaroo?', 'punchline': 'You get the breed to sign something for you! (Kangaroos are typically male, and we only cross males once in a while). So', 'score': 0}\n"
     ]
    }
   ],
   "source": [
    "print('A joke with a human-generated punchline:')\n",
    "print(dataset['test'][0])\n",
    "print()\n",
    "print('A joke with a GPT2-generated punchline:')\n",
    "print([x for x in dataset['test'] if x['score']==0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be53482b-ea00-4f19-8e5e-069884e6eb9f",
   "metadata": {},
   "source": [
    "<div style=background-color:#EEEEFF>\n",
    "\n",
    "Now that we've loaded our data, let's use it to train a model!\n",
    "    \n",
    "We're not, however, going to start training from scratch.  \n",
    "    \n",
    "Transformer models, each with 100s of millions or even 100s of *billions* of free parameters trained on an enormous corpus of documents, are very expensive to train, in terms of both time and compute costs.  Not only do we not want to wait through and pay for all that compute time, but the carbon footprint of training a state-of-the-art model is [LARGE](https://huggingface.co/course/chapter1/4?fw=pt#transformers-are-big-models).  \n",
    "    \n",
    "Instead, we will use [transfer learning](https://towardsdatascience.com/cnn-transfer-learning-fine-tuning-9f3e7c5806b2).  To do that, we start from a pre-trained model, which has already been trained through many epochs on huge document datasets.  We will then do some \"fine-tuning\" training using our Jokes dataset to produce a classifier that is particularly good at distinguishing human-generated vs. GPT2-generated jokes.\n",
    "    \n",
    "Let's start with the [BERT model](https://arxiv.org/pdf/1810.04805.pdf) from Google's AI Language lab.  We're going to use it to do \"sequence classification\", where the model decides if one sequence (in our case, the punchline) is an appropriate follow-on from a previous sequence (in our case, the setup)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ecc012c9-ec2d-428c-82fc-bf6f2f561422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using checkpoint \"bert-base-uncased\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using eos_token, but it is not set yet.\n",
      "Using bos_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: no EOS token detected.  Adding an EOS token\n"
     ]
    }
   ],
   "source": [
    "import model_tools as mtools\n",
    "\n",
    "checkpoint, tokenizer, model = mtools.load_model('bert')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647bcc09-6bc6-49ab-a05f-c34c15f785bd",
   "metadata": {},
   "source": [
    "<div style=background-color:#EEEEFF>\n",
    "\n",
    "We got some warnings that we've loaded a version of BERT that is not already set up for sequence classification, and that it needs some training to be ready to use this way.  That's okay!  Training is exactly what we are about to do.\n",
    "    \n",
    "But first, we need to tokenize the data using BERT's tokenizer, so that our text is encoded using the same token mapping that BERT has been pre-trained to expect.  \n",
    "    \n",
    "Two additional processes will happen as part of this tokenization: we'll pad the tokenized strings in each batch to be the same length, so that they can be loaded together into a single PyTorch tensor, and we'll generate the associated attention mask that tells the model which tokens are padding tokens that can be ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ac068d4-45f2-48c3-950d-465abddd5e25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/jupyter-genevievegraves/.cache/huggingface/datasets/csv/default-22e578e895d47822/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a/cache-f425e4cb13baf4a4.arrow\n",
      "Loading cached processed dataset at /home/jupyter-genevievegraves/.cache/huggingface/datasets/csv/default-22e578e895d47822/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a/cache-4e41e1f440a3c4e0.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['attention_mask', 'input_ids', 'punchline', 'score', 'setup', 'token_type_ids'],\n",
      "        num_rows: 2056\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['attention_mask', 'input_ids', 'punchline', 'score', 'setup', 'token_type_ids'],\n",
      "        num_rows: 881\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the data\n",
    "\n",
    "import data_tools as dtools\n",
    "\n",
    "# Use a tokenize function to deal with tokenization and (batch) padding:\n",
    "#    -- all tokenized strings in a batch need to be padded to the same length \n",
    "#       to be loaded into a PyTorch tensor together\n",
    "def tokenize_function(example):\n",
    "    full_qa = dtools.joke_as_qa(example['setup'], example['punchline'])\n",
    "    q = [x[:x.find('Answer:')].strip() for x in full_qa]\n",
    "    a = [x[x.find('Answer:'):].strip() for x in full_qa]\n",
    "    return tokenizer(q, a, padding=\"max_length\", max_length=60, truncation=True)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "print(tokenized_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e34e77-105d-47b6-8b85-a6ac5dfaf199",
   "metadata": {},
   "source": [
    "<div style=background-color:#EEEEFF>\n",
    "\n",
    "Notice that the tokenization process has generated and added the token \"input_ids\", \"token_type_ids\", and \"attention_mask\" to our data structure.  These features are what gets input to the model training process.\n",
    "    \n",
    "We also need to add the classification labels that we use to distinguish \"real\" from \"fake\" jokes.  In our case, the \"real\" data all have *score>0*, while \"fake\" data have *score=0*, so we will map everything with *score>0* to have *label=1* and everything with *score=0* to have *label=0*.\n",
    "    \n",
    "Finally, we want to drop the input columns from the dataset, now that we've generated the token lists, attention masks, and labels that we will pass to the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88efba32-8799-4a93-ac78-dd0734de7314",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/jupyter-genevievegraves/.cache/huggingface/datasets/csv/default-22e578e895d47822/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a/cache-822dfa901264c453.arrow\n",
      "Loading cached processed dataset at /home/jupyter-genevievegraves/.cache/huggingface/datasets/csv/default-22e578e895d47822/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a/cache-0c896bf6cf0df668.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['attention_mask', 'input_ids', 'labels', 'token_type_ids'],\n",
      "        num_rows: 2056\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['attention_mask', 'input_ids', 'labels', 'token_type_ids'],\n",
      "        num_rows: 881\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "tokenized_datasets = tokenized_datasets.map(lambda batch: {\"labels\": [int(x > 0) for x in batch[\"score\"]]}, batched=True)\n",
    "\n",
    "# Clean up / reformat data to fit into a PyTorch DataLoader\n",
    "# We don't need the text strings themselves anymore\n",
    "tokenized_datasets = tokenized_datasets.remove_columns([\"setup\", \"punchline\", \"score\"])\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "\n",
    "print(tokenized_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc43dfa-be99-4dfe-8252-453adf92bb14",
   "metadata": {},
   "source": [
    "<div style=background-color:#EEEEFF>\n",
    "\n",
    "Now we're ready to do the fine-tuning training of our classifier!  The training loop itself is implemented in the *train_classifier()* function defined in [model_tools.py](model_tools.py).\n",
    "    \n",
    "We'll train through just 3 epochs with our downsampled training set, just to see how well BERT does with minimal training.  The fake jokes looked pretty different from the real ones, so it shouldn't be too hard!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1812b626-fe16-4411-a14d-c73a34d3d1f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 257/771 [00:26<00:53,  9.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.018, Accuracy: 0.967, F1: 0.968\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 514/771 [00:58<00:27,  9.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 0.070, Accuracy: 0.952, F1: 0.951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 771/771 [01:30<00:00,  9.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Loss: 0.010, Accuracy: 0.972, F1: 0.971\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 771/771 [01:34<00:00,  8.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model as models/ClassifyJokes_bert_0.01subset_2021-12-17.pt\n"
     ]
    }
   ],
   "source": [
    "model = mtools.train_classifier(tokenized_datasets, model, epochs=3)\n",
    "\n",
    "from datetime import datetime\n",
    "checkpoint_name = checkpoint.split('/')[-1].split('-')[0]\n",
    "filename = 'models/ClassifyJokes_{}_{:4.2f}subset_{}'.format(checkpoint_name,1.0/downsample_factor,datetime.now().date())+'.pt'\n",
    "\n",
    "from torch import save\n",
    "print('Saving model as {}'.format(filename))\n",
    "save(model,filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068435df-de1b-49d1-8390-a1f88ed45d60",
   "metadata": {},
   "source": [
    "<div style=background-color:#EEEEFF>\n",
    "\n",
    "As expected, the classifier does pretty well with minimal training: better than 95%.  It may not even be able to improve much with the full training set.  It takes ~30 seconds/epoch to run on 1% of the full training set, so it should take a couple of hours to run 3 epochs with the full training set. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983b45a0-bf08-4c63-b6a5-9be1f995404e",
   "metadata": {},
   "source": [
    "<div style=background-color:#EEEEFF>\n",
    "\n",
    "The steps here have been encapsulated in the function *classify_punchlines()*, implemented in [punchline_classifier.py](punchline_classifier.py), so you can run the entire process documented above with the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e36d6b44-c694-46b0-85ab-94d802a3754e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from punchline_classifier import classify_punchlines\n",
    "\n",
    "# train_files = ['data/short_jokes_train.csv','data/short_jokes_train_fake.csv']\n",
    "# test_files = ['data/short_jokes_test.csv','data/short_jokes_test_fake.csv']\n",
    "\n",
    "# model = classify_punchlines(train_files, test_files, downsample=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc04387-6ab3-4998-a36d-551b5aa9550a",
   "metadata": {},
   "source": [
    "<div style=background-color:#EEEEFF>\n",
    "\n",
    "Using the entire dataset to train the classifier takes a couple of hours and gives only marginal improvement (~98% accuracy).  If you want to run it, we recommend running it from the command line in a detached screen, as in [2.FakePunchlines](2.FakePunchlines.ipynb).\n",
    "\n",
    "* `$> screen -S train_class`\n",
    "* `$> python punchline_classifier.py --train data/short_jokes_train.csv,data/short_jokes_train_fake.csv --test data/short_jokes_test.csv,data/short_jokes_test_fake.csv`\n",
    "\n",
    "Then \"Ctl-a d\" to detach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1040dcc-020d-4fde-add2-00b6918a78ee",
   "metadata": {},
   "source": [
    "<div style=background-color:#EEEEFF>\n",
    "\n",
    "When the model is finished running, we can load it with the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f9eead96-43ab-4f95-9f9a-58757bba95ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import load\n",
    "model = load(filename)\n",
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "38d4bde7-144a-4154-9ba9-f6f387539a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred = mtools.classify_punchlines(tokenized_datasets['test'],model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f62f65-c4e3-40d5-9fcc-28ac70cbb50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = list(tokenized_datasets['test']['labels'].squeeze().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f69bff-a7e5-40b7-b456-b55ffb1fd77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame()\n",
    "df['labels'] = labels\n",
    "df['pred'] = pred\n",
    "df['saved_pred'] = saved_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d23828a-e21b-4b07-b8da-240e52a7e7d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred      0    1\n",
      "labels          \n",
      "0       431    9\n",
      "1        16  425\n"
     ]
    }
   ],
   "source": [
    "print(df.groupby(['labels','pred']).size().unstack(fill_value=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233753e4-2a33-4051-8c4f-3107012c444b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved_pred    0    1\n",
      "labels              \n",
      "0           432    8\n",
      "1             8  433\n"
     ]
    }
   ],
   "source": [
    "print(df.groupby(['labels','saved_pred']).size().unstack(fill_value=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e727cb6-7bd2-48b2-ad29-77df53ab4176",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
